{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4b - Kolmogorov Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import pickle\n",
    "import math\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will explore how compression algorithms can be used to approximate the Kolmogorov complexity of datasets and learning tasks.\n",
    "\n",
    "\n",
    "\n",
    "Kolmogorov complexity $K(x)$ of an object $x$ (typically a string) is the length of the shortest possible program that can produce that object $x$ as output. Unfortunately it is an uncomputable quantity in the general case, but we can approximate it using practical compression tools such as zip. The compressors exploit patterns, redundancy, and structure in data — just as machine learning models do.\n",
    "\n",
    "This connection leads to a fascinating interpretation: learning can be viewed as compression. In both unsupervised and supervised learning, the goal is to describe data as compactly as possible, either by identifying structure in the data itself or by predicting labels from features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper bounds of the Kolmogorov complexity $K$ can be obtained by considering various encoders (=compressors). Given an encoder $h$, and data $x$ to compress, the bound on $K(x)$ is given by the size of the compressed data $h(x)$ + the size of the decoder $h^{-1}$, since one needs to describe both of them to re-generate the original data. \n",
    "\n",
    "Indeed, given data $x$ to encode (e.g. whole Wikipedia), imagine an encoder that associates that particular $x$ to just one bit, and applies zip otherwise (i.e. to any other string). The encoding of $x$ will be very short, but it is just because $x$ is actually stored in the decoder.\n",
    "\n",
    "Moreover, in the case of lossy encoders, i.e. that are not lossless (e.g., jpg encoder for images, or machine learning models in general), then one has to take into account that the decoder $h^{-1}$ will not produce the original data $x$ but a corrupted version $x' = h^{-1} \\circ h(x)$ (note the slight abuse of notation: $h^{-1}$ is the decoder, not exactly matching the encoder $h$ as the latter is not injective, so in general $h^{-1} \\circ h \\neq Id$). Therefore, one needs in plus to describe the residuals $z = x'-x$, in order to be able to generate the original data *exactly*. Given the decoded reconstruction $x'$, describing $z$ or $x$ is equivalent. The associated description cost is $K(z|x') = K(z|h^{-1}, h(x)) = K(x|h^{-1}, h(x))$.\n",
    "\n",
    "As a result, in the general case we have:\n",
    "$$ K(x) \\leq K(h^{-1}) + K( h(x) ) + K(x|h^{-1}, h(x))$$\n",
    "that is, the cost of describing the decoder, the cost of describing the encoded data (which is upper bounded by its length + twice its $\\log$), and the cost of describing residuals if the encoder is lossy.\n",
    "\n",
    "\n",
    "Note that Kolmogorov complexity does not include any computational complexity consideration (all programs, whether fast or very slow, are treated equally).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `unsupervised learning`, we are primarily interested in describing the structure of the data itself, such as clustering or density estimation. The goal is to find the shortest description of the dataset $x$. Considering lossless compression only, there is no residuals to describe. Thus, we have for instance:\n",
    "$$ K(x) \\leq |\\text{zip}(x)| + |\\text{unzip program}|$$\n",
    "\n",
    "In `supervised learning`, we do not only describe the inputs $x$, but also learn a predictor, i.e. a mapping from $x$ to the labels $y$, which makes mistakes. Here, the goal is to describe $y$ given $x$, using a model $h$. We will consider that for any sample $x_i$, the model outputs a distribution of probability $p_h(y|x_i)$ over possible labels. As the inputs $x$ are given to the predictor, they do not need to be encoded, and in the Kolmogorov complexity, only two terms remain: the description of the model (seen as a decoder) and the description of the true labels $y$ given the predictions $p_h(y|x_i)$ (similarly to residuals):\n",
    "$$ K(y|x) \\leq K(h) + K(y|h,x) $$\n",
    "where $K(y|h,x)$ stands for the log likelihood error of our model (over the whole dataset) given the data $x$, that is, $\\sum_i -\\log p_h(y_i|x_i)$, which is precisely the number of bits of information one should give to identify, for each input sample $x_i$, the true label $y_i$ given the prediction $p_h(y_i|x_i)$ of the model.\n",
    "\n",
    "This machine learning instanciation of the Kolmogorov complexity is named the _Minimum Description Length (MDL)_ paradigm.\n",
    "\n",
    "The Kolmogorov complexity $K(h)$ of the model itself can be approximated using the Bayesian Information Criterion (BIC) or the Akaike Information Criterion (AIC), based on the number of parameters of the models (and possibly the number of samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical\n",
    "1. Use `gzip` to compute a Kolmogorov complexity upper bound, assuming an unsupervised task on the dataset features. The data should be turned into binary first using pickle.\n",
    "\n",
    "2. Train the models given on the dataset.\n",
    "\n",
    "3. Use the trained models to compute Kolmogorov complexity upper bounds in the supervised setting. For MDL you should consider both the Bayesian and Akaike criterions. It is up to you to find the correct number of parameters for its model.\n",
    "\n",
    "4. How do the upper bounds compare between the different supervised models? Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model | k (params) |  −log₂L  (bits) | K(h) AIC  (bits) | K(h) BIC  (bits) | K(x,y) AIC (bits) | K(x,y) BIC (bits)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "      Linear Regression |         11 |          2749.9 |          5531.6 |          5592.9 |        105795.6 |        105856.9\n",
      "          Decision Tree |        691 |         -3967.9 |         -5942.1 |         -2087.6 |         94321.9 |         98176.4\n",
      " Support Vector Machine |       3873 |          2894.6 |         16964.3 |         38568.4 |        117228.3 |        138832.4\n",
      "    k-Nearest Neighbors |       3530 |          2717.8 |         15621.0 |         35311.8 |        115885.0 |        135575.8\n",
      "\n",
      "Unsupervised gzip bound on K(x)            : 100,264 bits\n",
      "Unsupervised gzip bound on K(x,y) (baseline): 107,688 bits\n"
     ]
    }
   ],
   "source": [
    "# Initialize Models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Support Vector Machine\": SVR(),\n",
    "    \"k-Nearest Neighbors\": KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# Function to compute compressed size as Kolmogorov complexity approximation\n",
    "def compute_compressed_size(obj):\n",
    "    pickled = pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    compressed = gzip.compress(pickled)\n",
    "    return len(compressed) * 8  # convert bytes → bits\n",
    "\n",
    "# TODO: compute Kolmogorov complextiy upper bounds\n",
    "def safe_nll_bits(y_true, y_pred, eps: float = 1e-8):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    sigma2 = max(mse, eps)\n",
    "    n = len(y_true)\n",
    "    nll_nat = n / 2 * (math.log(2 * math.pi * sigma2) + 1)\n",
    "    return nll_nat / math.log(2)  # nat → bits\n",
    "\n",
    "\n",
    "def num_parameters(model, name: str, X_sample):\n",
    "    if name == \"Linear Regression\":\n",
    "        return model.coef_.size + 1  # β plus intercept\n",
    "    if name == \"Decision Tree\":\n",
    "        return model.tree_.node_count # each node stores a threshold & feature index\n",
    "    if name == \"Support Vector Machine\":\n",
    "        return (model.support_vectors_.size +\n",
    "                model.dual_coef_.size + 1)\n",
    "    if name == \"k-Nearest Neighbors\":\n",
    "        return X_sample.size\n",
    "    raise ValueError(f\"Unknown model type: {name}\")\n",
    "\n",
    "compressed_X_bits = compute_compressed_size(X_train)\n",
    "compressed_xy_bits = compute_compressed_size((X_train, y_train))\n",
    "\n",
    "results = []  # to store everything for a small recap table\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "\n",
    "    nll_bits = safe_nll_bits(y_train, y_pred_train)\n",
    "    k = num_parameters(model, name, X_train)\n",
    "    n = len(X_train)\n",
    "\n",
    "    nll_nat = nll_bits * math.log(2)\n",
    "\n",
    "    aic_nat = 2 * k + 2 * nll_nat\n",
    "    bic_nat = k * math.log(n) + 2 * nll_nat\n",
    "\n",
    "    aic_bits = aic_nat / math.log(2)\n",
    "    bic_bits = bic_nat / math.log(2)\n",
    "\n",
    "    K_xy_AIC = compressed_X_bits + aic_bits\n",
    "    K_xy_BIC = compressed_X_bits + bic_bits\n",
    "\n",
    "    results.append(\n",
    "        (name, k, nll_bits, aic_bits, bic_bits,\n",
    "         K_xy_AIC, K_xy_BIC)\n",
    "    )\n",
    "\n",
    "header = (\"Model\", \"k (params)\", \"−log₂L  (bits)\",\n",
    "          \"K(h) AIC  (bits)\", \"K(h) BIC  (bits)\",\n",
    "          \"K(x,y) AIC (bits)\", \"K(x,y) BIC (bits)\")\n",
    "print(\"{:>23s} | {:>10s} | {:>15s} | {:>15s} | {:>15s} | {:>15s} | {:>15s}\".format(*header))\n",
    "print(\"-\"*120)\n",
    "for row in results:\n",
    "    print(\"{:>23s} | {:10d} | {:15.1f} | {:15.1f} | {:15.1f} | {:15.1f} | {:15.1f}\".format(*row))\n",
    "\n",
    "print(\"\\nUnsupervised gzip bound on K(x)            : {:,.0f} bits\".format(compressed_X_bits))\n",
    "print(\"Unsupervised gzip bound on K(x,y) (baseline): {:,.0f} bits\".format(compressed_xy_bits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in the supervised case we ommit the complexity of the data itself. This is because we are not interested in compressing the data, but rather in compressing the labels given the data. However, in order to have a fair comparisson with the unsupervised case, we will add this complexity to the mix. Thus, the consider upper bounds on the joint distribution $(x,y)$ instead of $y|x$:\n",
    "$$ K(x,y) \\leq K(h) + K(x) + K(y|h,x)$$\n",
    "and we approximate $K(x)$ by $|\\text{zip}(x)| $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Recompute the upper bound in the supervised case using the new formula.\n",
    "\n",
    "6. Now that the results are comparable, how do the upper bounds behave between the supervised and the unsupervised setting?\n",
    "\n",
    "7. Why is the zip compression so bad when dealing with tabular data?\n",
    "\n",
    "8. What are the assumptions behind using AIC and BIC for estimating the description length of a model? What types of models or settings might violate these assumptions?\n",
    "\n",
    "9. In the supervised setting, why is it important to separate the complexity of the model from the complexity of the residuals? What does this decomposition tell us ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n",
    "\n",
    "**5. Recompute the upper bound in the supervised case using the new formula.**  \n",
    "The joint‑MDL bound  \n",
    "$$\n",
    "K(x,y)\\;\\le\\;K(h)\\;+\\;K(x)\\;+\\;K(y\\,|\\,h,x)\n",
    "$$  \n",
    "adds the gzip description of the features, **K(x)= 100,264 bits**, to each model’s two‑part AIC/BIC length.  \n",
    "The numbers after this addition are:\n",
    "\n",
    "| Model | **K(x,y) AIC** | **K(x,y) BIC** |\n",
    "|-------|----------------|----------------|\n",
    "| Linear Regression | **105,796 bits** | **105,857 bits** |\n",
    "| Decision Tree | **94,322 bits** | **98,176 bits** |\n",
    "| Support Vector Machine | **117,228 bits** | **138,832 bits** |\n",
    "| k‑Nearest Neighbors | **115,885 bits** | **135,576 bits** |\n",
    "\n",
    "(The unsupervised baselines remain: gzip K(x)= 100,264 bits, gzip K(x,y)= 107,688 bits.)\n",
    "\n",
    "---\n",
    "\n",
    "**6. Now that the results are comparable, how do the upper bounds behave between the supervised and the unsupervised setting?**  \n",
    "* **Decision Tree** beats both unsupervised benchmarks by a large margin (≈ 13 kbits [AIC] or ≈ 9 kbits [BIC]), proving that a model‑plus‑residual code can compress better than generic zip.  \n",
    "* **Linear Regression** performs slightly better (≈ 1.8 kbits) than gzip K(x,y).  \n",
    "* **k‑NN** and **SVR** are worse than gzip because their massive decoders add more bits than they save in residuals.  \n",
    "Hence learning helps only when the model is expressive and compact enough to overcome its own description cost.\n",
    "\n",
    "---\n",
    "\n",
    "**7. Why is the zip compression so bad when dealing with tabular data?**  \n",
    "* **Binary noise.** Floats differ in low‑order bits; identical byte substrings—what gzip needs—are rare.  \n",
    "* **Structure ignorance.** gzip sees a flat stream, not rows and columns; correlations across columns/rows are invisible.  \n",
    "* **No semantic transforms.** It cannot delta‑code, quantise, or exploit linear relations; specialised columnar codecs or statistical models can.\n",
    "\n",
    "---\n",
    "\n",
    "**8. What are the assumptions behind using AIC and BIC for estimating the description length of a model? What types of models or settings might violate these assumptions?**  \n",
    "\n",
    "| Criterion | Key assumptions | Violations / risky settings |\n",
    "|-----------|-----------------|-----------------------------|\n",
    "| **AIC** | i.i.d. data, large‑sample asymptotics, candidate models near the truth | Heavy regularisation, small‑n data, autocorrelated time‑series, over‑parameterised deep nets |\n",
    "| **BIC** | All AIC assumptions plus \"true model is in the set\" and fixed finite parameter count k | Non‑parametric/lazy models (k‑NN), kernel/GP methods, trees whose node count grows with n, deep networks |\n",
    "\n",
    "When these premises fail, the penalties (2*k* for AIC, *k* log n for BIC) cease to represent a true description‑length contribution.\n",
    "\n",
    "---\n",
    "\n",
    "**9. In the supervised setting, why is it important to separate the complexity of the model from the complexity of the residuals? What does this decomposition tell us ?**  \n",
    "* **Interpretation.** *K(h)* measures how elaborate the rule is; *K(y | h,x)* measures the information the rule still misses.  \n",
    "* **Over‑fit control.** A larger model is worthwhile only if it cuts more bits from the residual message than it adds to K(h).  \n",
    "* **Bias–variance view.** K(h) ↔ capacity/bias, residual code ↔ unexplained variance/noise.  \n",
    "* **MDL principle.** The best hypothesis minimises the sum, aligning statistical learning with optimal lossless compression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
