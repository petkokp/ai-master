{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 - Compression, Prediction, Generation: Text Entropy\n",
    "\n",
    "#### Stella Douka, Guillaume Charpiat \n",
    "\n",
    "#### Credits: Gaétan Marceau-Caron, Francesco Pezzicoli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In this TP we are interested in compressing and generating texts written in natural languages.\n",
    "Given a text of length $n$, a sequence of symbols is just a vector $(x_1, . . . , x_n)$ where each $x_i$ is a symbol i.e. $x_i = a, b, c, \\dots$. We can define the alphabet of possible symbols as $\\mathcal{A} = \\{a_1,a_2,\\dots,a_M\\}$ then each $x_i$ can have $M$ values.\n",
    "\n",
    "In order to model the sequence of symbols we need a joint probability distribution for each symbol in the sequence, namely $p(X_1 = x_1, X_2 = x_2, \\dots , X_n = x_n)$. If our alphabet had $M$ symbols, for modelling a sequence of length $n$ we would need $M^n$ probabilities. Thus some assumptions are required in order to reduce this dimensionality. In this case we will use two different models for $p$, the IID and the Markov Chain model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXFy0UMH_jDH"
   },
   "source": [
    "### IID Model\n",
    "The IID model assumes:\n",
    "\n",
    "$$ p(X_1 = x_1, X_2 = x_2, \\dots , X_n = x_n) = \\prod_{i=1}^n p(X_i = x_i)$$\n",
    "\n",
    "i.e. that the symbols in a sequence are independent and identically distributed. With this model we need only $M$ probabilities, one for each symbol. One can generalize and use symbols not of a single character but of multiples ones. For example using 3 characters per symbol, the symbols would be of the form $aaa,aab,...,zzz$. When using $k$ characters per symbols in an alphabet of $M$ characters, the needed probabilities would be $M^k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsX7DGxl5pF7"
   },
   "source": [
    "### Markov Chain Model\n",
    "\n",
    "The Markov Chain model assume a limited range of dependence of the symbols. Indeed for an order $k$ Markov Chain:\n",
    "\n",
    "\n",
    "$$p(X_i | X_{i-1},X_{i-2},\\dots,X_1) = p(X_i | X_{i-1},X_{i-2},\\dots,X_{i-k})$$\n",
    "\n",
    "\n",
    "The meaning of the above structure is that the $i$-th symbol in the sequence depends only on the previous $k$ symbols. We add the time *invariant assumption*, meaning that the conditional probabilities do not depend on the time index $i$ i.e. $p(X_i | X_{i-1},X_{i-2},\\dots,X_{i-k}) = p(X_{k+1} | X_{k},X_{k-1},\\dots,X_{1})$. The most common and widely used Markov Chain is the Markov Chain of order 1:\n",
    "\n",
    "$$p(X_i | X_{i-1},X_{i-2},\\dots,X_1) = p(X_i | X_{i-1})$$\n",
    "\n",
    "In this case the conditional probability $p(X_i|X_{i−1})$ can be expressed using $M^2$\n",
    "numbers. Usually this is referred to as the *transition matrix*. Given an alphabet $\\mathcal{A} = \\{a_1,a_2,\\dots,a_M\\}$ the transition matrix can be written as: \n",
    "\n",
    "$$ \\mathbb{M}_{kl} = p(X_i = a_k| X_{i-1} = a_l) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and Cross-Entropy\n",
    "\n",
    "\n",
    "- For the IID model of order 1 the entropy computation is straightforward: \n",
    "$$ H_{IID} = -\\sum_{i=1}^M p(a_i) log p(a_i)$$ \n",
    "and consequently, starting from two distributions $p,q$ fitted on two different texts, the cross-entropy:\n",
    "$$ CE_{IID} = -\\sum_{i=1}^M p(a_i) log q(a_i)$$\n",
    "\n",
    "\n",
    "- For the MC model of order 1 the entropy is defined as follows: \n",
    "$$ H_{MC} = - \\sum_{kl} \\pi(a_k) p(X_i = a_k| X_{i-1} = a_l) log \\left(p(X_i = a_k| X_{i-1} = a_l)\\right)= - \\sum_{kl} \\pi_k\\mathbb{M}_{kl} log \\mathbb{M}_{kl}$$\n",
    "where $\\pi$ is the stationary distribution of the Markov Chain i.e. $\\pi_k = \\mathbb{M}_{kl} \\pi_l$. The code to compute the stationary distribution is already given.\n",
    "The cross-entropy:\n",
    "$$ CE_{IID} = - \\sum_{kl} \\pi_k\\mathbb{M}_{kl} log \\mathbb{M'}_{kl}$$\n",
    "with $\\mathbb{M}$ and $\\mathbb{M'}$ are fitted on two different texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Questions: \n",
    "\n",
    "1) Interpret the time invariant assumption associated to our Markov chains in the contex of text generation.\n",
    "\n",
    "2) How can we rewrite a Markov chain of higher order as a Markov chain of order 1?\n",
    "\n",
    "3) Given a probability distribution over symbols, how to use it for generating sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5d365",
   "metadata": {},
   "source": [
    "1.  **Interpret the time invariant assumption associated to our Markov chains in the context of text generation.**\n",
    "    The time-invariant assumption means that the probability of a particular symbol appearing depends *only* on the preceding *k* symbols (for an order *k* Markov chain), regardless of *where* in the sequence (or text) this pattern occurs. In text generation, this implies that the rules governing which character is likely to follow a specific sequence of *k* characters (e.g., the probability of 'e' following 'th') are considered constant throughout the entire text being generated. The likelihood of a transition doesn't change whether it's at the beginning, middle, or end of the text.\n",
    "\n",
    "2.  **How can we rewrite a Markov chain of higher order as a Markov chain of order 1?**\n",
    "    In Markov chains of order *k*, the probability of the current symbol depends on the *k* preceding symbols ($p(X_i | X_{i-1},X_{i-2},\\dots,X_{i-k})$). In the specific case of an order 1 Markov chain, the dependency is only on the single preceding symbol ($p(X_i | X_{i-1})$). By redefining the state space to include sequences of past states, an \\(n\\)-order Markov chain can be represented as a first-order Markov chain.\n",
    "\n",
    "3.  **Given a probability distribution over symbols, how to use it for generating sentences?**\n",
    "    We can use two approaches to model the probability of sequences of symbols:\n",
    "    * **IID Model:** Assigns an independent probability $p(X_i = a_i)$ to each symbol $a_i$.\n",
    "    * **Markov Chain Model:** Assigns conditional probabilities $p(X_i | X_{i-1},X_{i-2},\\dots,X_{i-k})$ (or just $p(X_i | X_{i-1})$ for order 1) based on preceding symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical questions\n",
    "\n",
    "In order to construct our IID and Markov Chain models we need some text. Our source will be a set of classical novels available at: https://www.lri.fr/~gcharpia/informationtheory/TP2_texts.zip\n",
    "\n",
    "We will use the symbols in each text to learn the probabilities of each model. The alphabet we suggest for the characters to use is string.printable which is made of $\\sim 100$ characters. (see below)\n",
    "\n",
    "For both models, perform the following steps:\n",
    "\n",
    "1) For different orders of dependencies, train the model on a novel and compute the associated entropy. What do you observe as the order increases? Explain your observations.\n",
    "\n",
    "2) Use the other novels as test sets and compute the cross-entropy for each model trained previously. How to handle symbols (or sequences of symbols) not seen in the training set?\n",
    "\n",
    "3) For each order of dependencies, compare the cross-entropy with the entropy. Explain and interpret the differences.\n",
    "\n",
    "4) Choose the order of dependencies with the lowest cross-entropy and generate some sentences.\n",
    "\n",
    "5) Train one model per novel and use the KL divergence in order to cluster the novels.\n",
    "\n",
    "\n",
    "<b>Hints</b> : \n",
    "\n",
    "- In the MC case limit yourself to order $2$ (the computation can become quite expensive). If you have $ M \\sim 100$ characters, for order $1$ you will need a $\\sim 100 \\times 100$ matrix, for order $2$ a $\\sim 10^4 \\times 10^4$ matrix.\n",
    "\n",
    "- For the second order MC model you need to compute: $p(X_{i+1},X_{i}|X_{i},X_{i-1})$\n",
    "\n",
    "- It is possible to implement efficiently the two models with dictionaries inPython.  For the IID model, a key of the dictionary is simply a symbol and the value is the number of occurrences of the symbol in the text. For a Markov chain, a key of the dictionary is also a symbol, but the value is a vector that contains the number of occurrences of each character of the alphabet.  Notice that a symbol may consist of one or several characters. Note also that there is no need to explicitly consider all possible symbols; the ones that are observed in the training set are sufficient.\n",
    "\n",
    "- A low probability can be assigned to symbols not observed in the training-set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing stationary distribution \n",
    "\n",
    "Here we provide you two version of the function to compute the stationary distirbution of a markov chain and show a small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#direct way to find pi (can be slow)\n",
    "import  numpy  as np\n",
    "\n",
    "def Compute_stationary_distribution(P_kl):\n",
    "    ## P_kl must be the transition matrix from state l to state k!\n",
    "    evals , evecs = np.linalg.eig(P_kl)   \n",
    "    evec1 = evecs[:,np.isclose(evals , 1)]\n",
    "    evec1 = evec1 [:,0]\n",
    "    pi = evec1 / evec1.sum()\n",
    "    pi = pi.real #stationary  probability\n",
    "    \n",
    "    return pi \n",
    "\n",
    "#iteative way (should be faster)\n",
    "def Compute_stationary_distribution_it(P_kl, n_it):\n",
    "    pi = np.random.uniform(size=P_kl.shape[0]) #initial state, can be a random one!\n",
    "    pi /= pi.sum()\n",
    "    #print(pi,pi.sum())\n",
    "    for t in range(n_it):   \n",
    "        pi = np.matmul(P_kl,pi)\n",
    "    \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.625, 0.375])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##simple example of computation of stationary distribution \n",
    "\n",
    "n_it = 1000                                     ##remind to check that n_it is enough to reach convergence\n",
    "P_kl = np.array([[0.7,0.5],[0.3,0.5]])\n",
    "Compute_stationary_distribution_it(P_kl,n_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Alphabet\n",
    "\n",
    "Example of uploading a text and filtering out characters which are not in the chosen alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  string\n",
    "\n",
    "def import_text(file_name):\n",
    "    lines = []\n",
    "    with  open(file_name , encoding='UTF8') as f:\n",
    "        lines = f.readlines ()\n",
    "        text = '\\n'.join(lines)\n",
    "        printable = set(string.printable)\n",
    "        text = ''.join(filter(lambda x: x in printable , text))     \n",
    "    return text\n",
    "\n",
    "text = import_text('./texts/Alighieri.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IID - MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict, Counter \n",
    "\n",
    "class IIDModel:\n",
    "    def __init__(self, order=1):\n",
    "        self.order = order\n",
    "        self.prob_dict = {}\n",
    "        self.alphabet = None\n",
    "        self.total_count = 0\n",
    "\n",
    "    def process(self, text):\n",
    "        counts = Counter(text)\n",
    "        self.total_count = sum(counts.values())\n",
    "\n",
    "        self.alphabet = list(set(text))\n",
    "\n",
    "        self.prob_dict = {}\n",
    "        for sym in self.alphabet:\n",
    "            self.prob_dict[sym] = counts[sym] / self.total_count\n",
    "\n",
    "    def getEntropy(self):\n",
    "        H = 0.0\n",
    "        for sym, p in self.prob_dict.items():\n",
    "            if p > 0:\n",
    "                H -= p * math.log(p, 2)\n",
    "        return H\n",
    "\n",
    "    def getCrossEntropy(self, text):\n",
    "        n = len(text)\n",
    "        if n == 0:\n",
    "            return 0.0\n",
    "\n",
    "        eps = 1e-8\n",
    "        V = len(self.alphabet)\n",
    "        logsum = 0.0\n",
    "        for x in text:\n",
    "            if x in self.prob_dict:\n",
    "                px = self.prob_dict[x]\n",
    "            else:\n",
    "                px = eps\n",
    "            logsum += math.log(px, 2)\n",
    "        CE = -logsum / n\n",
    "        return CE\n",
    "\n",
    "    def generate(self, length):\n",
    "        if not self.prob_dict:\n",
    "            return \"\"\n",
    "        import random\n",
    "        symbols = list(self.prob_dict.keys())\n",
    "        probs   = list(self.prob_dict.values())\n",
    "        out = []\n",
    "        for _ in range(length):\n",
    "            out.append(random.choices(symbols, weights=probs, k=1)[0])\n",
    "        return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##clustering texts \n",
    "\n",
    "def KL_divergence(dist1, dist2):\n",
    "    kl = 0.0\n",
    "    for x, p1 in dist1.items():\n",
    "        if p1 <= 0:\n",
    "            continue\n",
    "        p2 = dist2.get(x, 1e-12)\n",
    "        kl += p1 * math.log(p1 / p2, 2)\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MARKOV CHAIN - MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovModel:\n",
    "    def __init__(self, order=2):\n",
    "        self.order = order\n",
    "        self.transitions = defaultdict(Counter)\n",
    "        self.alphabet = None\n",
    "        self.transition_probs = defaultdict(dict)\n",
    "\n",
    "    def process(self, text):\n",
    "        if len(text) < self.order:\n",
    "            return\n",
    "\n",
    "        self.alphabet = list(set(text))\n",
    "\n",
    "        if self.order == 1:\n",
    "            for i in range(len(text) - 1):\n",
    "                state = text[i]\n",
    "                nxt   = text[i+1]\n",
    "                self.transitions[state][nxt] += 1\n",
    "\n",
    "        elif self.order == 2:\n",
    "            for i in range(len(text) - 2):\n",
    "                state = (text[i], text[i+1])\n",
    "                nxt   = text[i+2]\n",
    "                self.transitions[state][nxt] += 1\n",
    "\n",
    "        for state, counter_next in self.transitions.items():\n",
    "            total = sum(counter_next.values())\n",
    "            for nxt_sym, cnt in counter_next.items():\n",
    "                self.transition_probs[state][nxt_sym] = cnt / total\n",
    "\n",
    "    def getEntropy(self):\n",
    "        if not self.transition_probs:\n",
    "            return 0.0\n",
    "\n",
    "        states = list(self.transition_probs.keys())\n",
    "        idx_map = {s: i for i, s in enumerate(states)}\n",
    "\n",
    "        n_states = len(states)\n",
    "        P_kl = np.zeros((n_states, n_states))\n",
    "\n",
    "        if self.order == 1:\n",
    "            for s in states:\n",
    "                i_s = idx_map[s]\n",
    "                for nxt_sym, pval in self.transition_probs[s].items():\n",
    "                    i_next = idx_map[nxt_sym] if nxt_sym in idx_map else None\n",
    "                    if i_next is not None:\n",
    "                        P_kl[i_next, i_s] = pval\n",
    "\n",
    "        elif self.order == 2:\n",
    "            for s in states:\n",
    "                i_s = idx_map[s]\n",
    "                (a, b) = s\n",
    "                for nxt_sym, pval in self.transition_probs[s].items():\n",
    "                    new_state = (b, nxt_sym)\n",
    "                    i_next = idx_map[new_state] if new_state in idx_map else None\n",
    "                    if i_next is not None:\n",
    "                        P_kl[i_next, i_s] = pval\n",
    "\n",
    "        pi = Compute_stationary_distribution_it(P_kl, 1000)\n",
    "\n",
    "        H = 0.0\n",
    "        for s in states:\n",
    "            i_s = idx_map[s]\n",
    "            pi_s = pi[i_s]\n",
    "            for nxt_sym, pval in self.transition_probs[s].items():\n",
    "                if pval > 0:\n",
    "                    H -= pi_s * pval * math.log(pval, 2)\n",
    "        return H\n",
    "\n",
    "    def getCrossEntropy(self, text):\n",
    "        n = len(text)\n",
    "        if self.order == 1:\n",
    "            if n < 2:\n",
    "                return 0.0\n",
    "            log_sum = 0.0\n",
    "            count_trans = 0\n",
    "            for i in range(n - 1):\n",
    "                state = text[i]\n",
    "                nxt   = text[i+1]\n",
    "                p = self.transition_probs.get(state, {}).get(nxt, 1e-8)\n",
    "                log_sum += math.log(p, 2)\n",
    "                count_trans += 1\n",
    "            return -log_sum / count_trans\n",
    "\n",
    "        elif self.order == 2:\n",
    "            if n < 3:\n",
    "                return 0.0\n",
    "            log_sum = 0.0\n",
    "            count_trans = 0\n",
    "            for i in range(n - 2):\n",
    "                state = (text[i], text[i+1])\n",
    "                nxt   = text[i+2]\n",
    "                p = self.transition_probs.get(state, {}).get(nxt, 1e-8)\n",
    "                log_sum += math.log(p, 2)\n",
    "                count_trans += 1\n",
    "            return -log_sum / count_trans\n",
    "\n",
    "    def generate(self, length):\n",
    "        if not self.transition_probs:\n",
    "            return \"\"\n",
    "        import random\n",
    "\n",
    "        states = list(self.transition_probs.keys())\n",
    "        out = []\n",
    "\n",
    "        if self.order == 1:\n",
    "            state = random.choice(states)\n",
    "            out.append(state)\n",
    "            for _ in range(length - 1):\n",
    "                next_dict = self.transition_probs[state]\n",
    "                if not next_dict:\n",
    "                    break\n",
    "                symbols = list(next_dict.keys())\n",
    "                probs   = list(next_dict.values())\n",
    "                nxt = random.choices(symbols, weights=probs, k=1)[0]\n",
    "                out.append(nxt)\n",
    "                state = nxt\n",
    "            return ''.join(out)\n",
    "\n",
    "        elif self.order == 2:\n",
    "            state = random.choice(states)\n",
    "            out.append(state[0])\n",
    "            out.append(state[1])\n",
    "            for _ in range(length - 2):\n",
    "                next_dict = self.transition_probs[state]\n",
    "                if not next_dict:\n",
    "                    break\n",
    "                symbols = list(next_dict.keys())\n",
    "                probs   = list(next_dict.values())\n",
    "                nxt = random.choices(symbols, weights=probs, k=1)[0]\n",
    "                out.append(nxt)\n",
    "                state = (state[1], nxt)\n",
    "            return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34b1e8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IID Entropy (training): 4.1898567285596116\n",
      "IID Cross-Entropy on itself: 4.189856728559201\n",
      "\n",
      "IID Generated Text (first 200 chars):\n",
      " ia danEaaroVtars og:a i u d oe\n",
      "smeOi\n",
      "col  nd eon \n",
      " te lcza  tfdnori\n",
      "eieu teetzacFsrio n\n",
      "\n",
      "ls\n",
      "\n",
      " ohnltuicnu,hsioial\n",
      "clile ll\n",
      "\n",
      "m ogeosrlher lUmnpad\n",
      " o\n",
      "c tfomt\n",
      "cc r pet\n",
      "ooolle\n",
      "me dusi mdtnoi urdiu anrl;com\n",
      "\n",
      "Markov(1) Entropy (training): 3.171667169429363\n",
      "Markov(1) Cross-Entropy on itself: 3.1716779288317674\n",
      "Markov(1) Generated Text (first 200 chars):\n",
      " Atrte co pemonalce colui roi.\n",
      "\n",
      " Quagr  covovenoin dagndome i  panzale\n",
      "\n",
      " frimmo manciaranterdior de\n",
      " nocel prllegncheraio,     Ditiano   l ola moici pa vora.\n",
      " pe so ve da che linacher a naso ma\n",
      "  ca pe\n",
      "\n",
      "Markov(2) Entropy (training): 2.611189845967297\n",
      "Markov(2) Cross-Entropy on itself: 2.611204100825836\n",
      "Markov(2) Generated Text (first 200 chars):\n",
      " be quarvea ca nossiersia que chEsta guanto ria!;\n",
      "\n",
      "  cognon s i suov , andon dalto milava vogna n se surgatte, quel per ch lidistre\n",
      "\n",
      "  pancia cote, se: Comento,\n",
      "\n",
      "\n",
      "  no ventu s mi\n",
      "\n",
      "  Fiello co crena me \n",
      "\n",
      "Cross-Entropy on Dostoevsky (IID):    5.593933231701195\n",
      "Cross-Entropy on Dostoevsky (M1):     7.900817668056938\n",
      "Cross-Entropy on Dostoevsky (M2):     11.908672436850944\n",
      "\n",
      "KL divergence Alighieri->Dostoevsky: 0.21921009499362426\n",
      "KL divergence Dostoevsky->Alighieri: 1.5816008806660617\n"
     ]
    }
   ],
   "source": [
    "iid_model = IIDModel(order=1)\n",
    "iid_model.process(text)\n",
    "H_iid = iid_model.getEntropy()\n",
    "print(\"IID Entropy (training):\", H_iid)\n",
    "\n",
    "CE_iid_self = iid_model.getCrossEntropy(text)\n",
    "print(\"IID Cross-Entropy on itself:\", CE_iid_self)\n",
    "\n",
    "gen_iid = iid_model.generate(200)\n",
    "print(\"\\nIID Generated Text (first 200 chars):\\n\", gen_iid)\n",
    "\n",
    "markov1 = MarkovModel(order=1)\n",
    "markov1.process(text)\n",
    "H_m1 = markov1.getEntropy()\n",
    "print(\"\\nMarkov(1) Entropy (training):\", H_m1)\n",
    "CE_m1_self = markov1.getCrossEntropy(text)\n",
    "print(\"Markov(1) Cross-Entropy on itself:\", CE_m1_self)\n",
    "\n",
    "gen_m1 = markov1.generate(200)\n",
    "print(\"Markov(1) Generated Text (first 200 chars):\\n\", gen_m1)\n",
    "\n",
    "markov2 = MarkovModel(order=2)\n",
    "markov2.process(text)\n",
    "H_m2 = markov2.getEntropy()\n",
    "print(\"\\nMarkov(2) Entropy (training):\", H_m2)\n",
    "CE_m2_self = markov2.getCrossEntropy(text)\n",
    "print(\"Markov(2) Cross-Entropy on itself:\", CE_m2_self)\n",
    "\n",
    "gen_m2 = markov2.generate(200)\n",
    "print(\"Markov(2) Generated Text (first 200 chars):\\n\", gen_m2)\n",
    "\n",
    "test_text = import_text(\"./texts/Dostoevsky.txt\")\n",
    "\n",
    "CE_iid_test = iid_model.getCrossEntropy(test_text)\n",
    "CE_m1_test = markov1.getCrossEntropy(test_text)\n",
    "CE_m2_test = markov2.getCrossEntropy(test_text)\n",
    "\n",
    "print(\"\\nCross-Entropy on Dostoevsky (IID):   \", CE_iid_test)\n",
    "print(\"Cross-Entropy on Dostoevsky (M1):    \", CE_m1_test)\n",
    "print(\"Cross-Entropy on Dostoevsky (M2):    \", CE_m2_test)\n",
    "\n",
    "iid_model_sh = IIDModel(order=1)\n",
    "iid_model_sh.process(test_text)\n",
    "kl_Alighieri_Sh = KL_divergence(iid_model.prob_dict, iid_model_sh.prob_dict)\n",
    "kl_Sh_Alighieri = KL_divergence(iid_model_sh.prob_dict, iid_model.prob_dict)\n",
    "\n",
    "print(\"\\nKL divergence Alighieri -> Dostoevsky:\", kl_Alighieri_Sh)\n",
    "print(\"KL divergence Dostoevsky -> Alighieri:\", kl_Sh_Alighieri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5cb485",
   "metadata": {},
   "source": [
    "1) As the Markov order goes from IID (order=0) to Markov(1) to Markov(2), the **entropy decreases** (e.g., from ~4.19 bits to ~3.17 bits to ~2.61 bits per character). The **higher the order**, the more context the model uses to predict the next symbol. This **increases predictability** (reduces uncertainty) and hence **lowers the entropy**. An IID model knows nothing about context; Markov(1) uses the previous symbol, and Markov(2) uses the previous two symbols, thereby reducing uncertainty even more.\n",
    "\n",
    "2) When testing the Alighieri-trained model on a *different* author (e.g., Dostoevsky), the cross-entropy can become **quite high**—especially for **Markov(2)**. This is because the **second-order** context patterns from Alighieri’s text rarely match Dostoevsky’s style. If the test text contains **new** symbols or transitions not present in training, the model would naively assign probability **zero**, causing infinite -log p. A potential solution is **smoothing** — assigning a small positive probability (e.g., 1e-8) to unseen events. This prevents cross-entropy from increasing to infinity.\n",
    "\n",
    "3) **Entropy** is computed on the same text the model was trained on, so it measures how well the model “explains” its **own** training data. **Cross-entropy** is measured on **new** text (potentially from a different distribution). Typically on the **training** text cross-entropy approximates entropy. On **unseen** text, cross-entropy is greater or equal to entropy. The gap reflects **distribution mismatch** (i.e., **KL divergence**). The bigger the mismatch, the larger the cross-entropy compared to the model’s training entropy. IID cross-entropy on Dostoevsky is 5.59 bits/char, vs. Markov(1) = 7.90, and Markov(2) = 11.90. For Markov(2) the cross-entropy is so high because the second-order model overfits more specific patterns from Alighieri’s style that do not appear in Dostoevsky. Consequently, it assigns many zero/near-zero probabilities, resulting in a large negative log-likelihood (hence large cross-entropy). In contrast, the IID model does not “specialize” as heavily and therefore generalizes somewhat better on a different style of text — thus a lower cross-entropy than Markov(1) or Markov(2).\n",
    "\n",
    "Overall, the results illustrate that the Markov(2) model is very “specialized” to Alighieri’s text, which is excellent for capturing nuances in that text, but relatively poor for modeling a different text (Dostoevsky). We also see that a higher order model achieves lower entropy on training (more context = less uncertainty). Other observations are that cross-entropy on unseen text is often higher, especially for specialized (Markov(2)) models (smoothing can handle unseen events) and that cross-entropy is greater or equal to entropy (the gap is KL divergence (distribution mismatch))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
