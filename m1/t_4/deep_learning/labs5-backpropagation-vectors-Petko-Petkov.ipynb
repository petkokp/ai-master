{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f73c13a-6a41-41a8-b3cb-bf0aaac86ebc",
   "metadata": {},
   "source": [
    "# Deep Learning - Lab Exercise 5\n",
    "\n",
    "\n",
    "**WARNING:** you must have finished the previous exercise before this one as you will re-use parts of the code.\n",
    "\n",
    "In the first lab exercise, we built a simple linear classifier.\n",
    "Although it can give reasonable results on the MNIST datasetÂ (~92.5% of accuracy), deeper neural networks can achieve more the 99% accuracy.\n",
    "However, it can quickly become really impracical to explicitly code forward and backward passes.\n",
    "Hence, it is useful to rely on an auto-diff library where we specify the forward pass once, and the backward pass is automatically deduced from the computational graph structure.\n",
    "\n",
    "In this lab exercise, we will build a small and simple auto-diff lib that mimics the autograd mechanism from Pytorch (of course, we will simplify a lot!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "576d2e25-2822-46e1-a6de-80f793898317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs that we will use\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# To load the data we will use the script of Gaetan Marceau Caron\n",
    "# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n",
    "import dataset_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8ce741f-b2a7-422d-96de-d71cb79cfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    # this link doesn't work any more,\n",
    "    # seach on google for the file \"mnist.pkl.gz\"\n",
    "    # and download it\n",
    "    !wget https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz\n",
    "\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4204abd-5cce-48f1-a78f-392a8275fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d35084-f93b-4245-b5cf-55baf1eb5f67",
   "metadata": {},
   "source": [
    "## Computation Graph\n",
    "\n",
    "Instead of directly manipulating numpy arrays, we will manipulate abstraction that contains:\n",
    "- a value (i.e. a numpy array)\n",
    "- a bool indicating if we wish to compute the gradient with respect to the value\n",
    "- the gradient with respect to the value\n",
    "- the operation to call during backpropagation\n",
    "\n",
    "There will be two kind of nodes:\n",
    "- ComputationGraphNode: a generic computation node\n",
    "- Parameter: a computation node that is used to store parameters of the network. Parameters are always leaf nodes, i.e. they cannot be build from other computation nodes.\n",
    "\n",
    "Our implementation of the backward pass will be really simple and incorrect in the general case (i.e. won't work with computation graph with loops).\n",
    "We will just apply the derivative function for a given tensor and then call the ones of its antecedents, recursively.\n",
    "This simple algorithm is good enough for this exercise.\n",
    "\n",
    "Note that a real implementation of backprop will store temporary values during forward that can be used during backward to improve computation speed. We do not do that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a1535415-f6ce-486a-850a-120cd9008642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationGraphNode(object):\n",
    "    \n",
    "    def __init__(self, data, require_grad=False):\n",
    "        # we initialise the value of the node and the grad\n",
    "        if(not isinstance(data, np.ndarray)):\n",
    "            data = np.array(data)\n",
    "        self.value = data\n",
    "        self.grad = None\n",
    "        \n",
    "        self.require_grad = require_grad\n",
    "        self.func = None\n",
    "        self.input_nodes = None\n",
    "        self.func_parameters = []\n",
    "    \n",
    "    def set_input_nodes(self, *nodes):\n",
    "        self.input_nodes = list(nodes)\n",
    "\n",
    "    def set_func_parameters(self, *func_parameters):\n",
    "        self.func_parameters = list(func_parameters)\n",
    "    \n",
    "    def set_func(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.grad is not None:\n",
    "            self.grad.fill(0)\n",
    "\n",
    "    def set_gradient(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulate gradient for this tensor\n",
    "        \"\"\"\n",
    "        if gradient.shape != self.value.shape:\n",
    "            print(gradient.shape, self.value.shape)\n",
    "            raise RuntimeError(\"Invalid gradient dimension\")\n",
    "        if self.grad is None:\n",
    "            self.grad = gradient\n",
    "        else:\n",
    "            self.grad += gradient\n",
    "    \n",
    "    def backward(self, g=None):\n",
    "        if g is None:\n",
    "            g = self.value.copy()\n",
    "            g.fill(1.)\n",
    "        self.set_gradient(g)\n",
    "        if self.func is not None:\n",
    "            grad_list = self.func.backward(*(self.input_nodes + self.func_parameters + [g]))\n",
    "            for input_node, ngrad in zip(self.input_nodes, grad_list):\n",
    "                input_node.backward(ngrad)\n",
    "    \n",
    "    def __add__(self, y):\n",
    "        if not isinstance(y, ComputationGraphNode):\n",
    "            y = ComputationGraphNode(y)\n",
    "        return Addition()(self, y)\n",
    "\n",
    "    def __getitem__(self, slice):\n",
    "        return Selection()(self, slice)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value.__str__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.value.__str__()\n",
    "\n",
    "class Parameter(ComputationGraphNode):\n",
    "    def __init__(self, data, name=\"default\"):\n",
    "        super().__init__(data, require_grad=True)\n",
    "        self.name  = name\n",
    "\n",
    "    def backward(self, g=None):\n",
    "        if g is not None:\n",
    "            self.set_gradient(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24dbd86-f592-4f8a-8657-272568a44f02",
   "metadata": {},
   "source": [
    "The class `Operation` is a class that three methods you should reimplement only the forward and the backward methods.\n",
    "* The `forward` method compute the function w.r.t inputs and return a new node that must contains information for backward pass.\n",
    "* The `backward` functions compute the gradient of the function w.r.t gradient of the output and other informations (forward pass input, parameter of the function...).**It should return a tuple**\n",
    "\n",
    "For better understanding below two operation are implemented, the selection and the addition (notice that it should not works yet since we do not defined what is a node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ca2caa2d-da91-4765-b709-b31acad0c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    @staticmethod\n",
    "    def forward(*args):\n",
    "        raise NotImplementedError(\"It is an abstract method\")\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        output_node = self.forward(*args)\n",
    "        output_node.set_func(self)\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(*args):\n",
    "        pass\n",
    "class Addition(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, y):\n",
    "        output_array = x.value + y.value\n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        output_node.set_input_nodes(x, y)\n",
    "        return output_node\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x, y, gradient):\n",
    "        return (gradient, gradient)\n",
    "\n",
    "class Selection(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, slice):\n",
    "        np_x = x.value\n",
    "\n",
    "        output_array = np_x.__getitem__(slice)\n",
    "        \n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        output_node.set_input_nodes(x)\n",
    "        output_node.set_func_parameters(slice)\n",
    "\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, slice, gradient):\n",
    "        np_x = x.value\n",
    "\n",
    "        cgrad = np_x.copy()\n",
    "        cgrad.fill(0)\n",
    "        cgrad.__setitem__(slice, gradient)\n",
    "        \n",
    "        return cgrad,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8a571-436a-499a-986a-f8a5ae89dab4",
   "metadata": {},
   "source": [
    "**Question 1** Complete the following class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3a3f6684-1b56-40ad-8fe4-7dd9373f81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        # we copy the value of the input node\n",
    "        np_x = x.value.copy()\n",
    "\n",
    "        # set negative elements to zero\n",
    "        np_x[np_x < 0] = 0 # notice we consider strictly < 0\n",
    "\n",
    "        # we create the output node needing only the node x\n",
    "        output_node = ComputationGraphNode(np_x)\n",
    "        output_node.set_input_nodes(x)\n",
    "\n",
    "        return output_node\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x, gradient):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the loss with respect to the input of ReLU.\n",
    "\n",
    "        Args:\n",
    "            x (ComputationGraphNode): The input node from the forward pass.\n",
    "            gradient (np.ndarray): The gradient flowing back from the next layer (dL/d(output)).\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the gradient with respect to the input x (dL/dx).\n",
    "        \"\"\"\n",
    "        # Get the value of the input node\n",
    "        np_x = x.value\n",
    "\n",
    "        # Create a mask where input was positive (derivative is 1)\n",
    "        # Elsewhere, the derivative is 0.\n",
    "        relu_derivative = (np_x > 0).astype(gradient.dtype) # Use same dtype as gradient\n",
    "\n",
    "        # Apply chain rule: dL/dx = dL/d(output) * d(output)/dx\n",
    "        grad_input = gradient * relu_derivative\n",
    "\n",
    "        # Return as a tuple (as required by the framework's backward structure)\n",
    "        return (grad_input,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82289abf-c8b1-495f-86b7-e8e27cd0cac3",
   "metadata": {},
   "source": [
    "We recall that :  $$tanh(x)= \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$$ \n",
    "\n",
    "However we can have stability issues if $||z||$ is large, e.g. $e^{10000}$ will lead to computation error or infinity. Indeed in python using numpy:\n",
    "\n",
    "\n",
    ">np.exp(10000)\n",
    "\n",
    "\n",
    "will leads to :\n",
    "\n",
    ">/tmp/ipykernel_7784/2473798304.py:1: RuntimeWarning: overflow encountered in exp\n",
    ">np.exp(10000)\n",
    ">\n",
    ">inf\n",
    "\n",
    "We can use the same tricks that the one used in the softmax computation observing the simple following fact: \n",
    "$$\n",
    "\\begin{aligned}\n",
    " tanh(x) &= \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} \\\\\n",
    " &= \\left(\\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\\right)\\frac{e^{-a}}{e^{-a}} \\\\\n",
    " &= \\frac{e^{z}e^{-a} - e^{-z}e^{-a}}{e^{z}e^{-a} + e^{-z}e^{-a}} \\\\\n",
    "&= \\frac{e^{z-a} - e^{-z-a}}{e^{z-a} + e^{-z-a}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Thus we want that $z-a$ or $-z-a$ be small, or in our case lower than $0$.  Thus taking $a$ as the absolute value of $z$ ($|z|$) will leads to have \n",
    "$z-a\\leq 0$ and $-z-a\\leq 0$.\n",
    "\n",
    "\n",
    "For the backward notice that $tanh'(x) = 1-\\sigma(x)^2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "89adae37-5902-4e50-ac29-5ed332cc0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(Operation):\n",
    "    @staticmethod\n",
    "    def TanHCompute(z):\n",
    "        \"\"\"Computes tanh(z) element-wise using a numerically stable approach.\"\"\"\n",
    "        if not isinstance(z, np.ndarray):\n",
    "             z = np.array(z) # Ensure input is a numpy array\n",
    "\n",
    "        # Numerical stability trick: subtract max(|z|) from exponents\n",
    "        abs_z = np.abs(z)\n",
    "        # Note: The explanation uses 'a = |z|', which implies element-wise absolute value.\n",
    "        # The subtraction happens inside the exp calls.\n",
    "\n",
    "        exp_pos = np.exp(z - abs_z)   # Corresponds to exp(z-a)\n",
    "        exp_neg = np.exp(-z - abs_z)  # Corresponds to exp(-z-a)\n",
    "\n",
    "        # Avoid division by zero if exp_pos + exp_neg is very small\n",
    "        # Although with the stabilization, this is less likely.\n",
    "        denominator = exp_pos + exp_neg\n",
    "        # Handle potential division by zero or very small denominators\n",
    "        # A small epsilon prevents division by zero and NaN results.\n",
    "        # Use np.finfo for a suitable small number based on the dtype.\n",
    "        epsilon = np.finfo(z.dtype).eps\n",
    "        tanh_val = (exp_pos - exp_neg) / (denominator + epsilon)\n",
    "\n",
    "        return tanh_val\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        \"\"\"Computes the forward pass for TanH.\"\"\"\n",
    "        output_array = TanH.TanHCompute(x.value)\n",
    "\n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        output_node.set_input_nodes(x)\n",
    "\n",
    "        return output_node\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x, gradient):\n",
    "        \"\"\"\n",
    "        Computes the gradient of the loss with respect to the input of TanH.\n",
    "\n",
    "        Args:\n",
    "            x (ComputationGraphNode): The input node from the forward pass.\n",
    "            gradient (np.ndarray): The gradient flowing back from the next layer (dL/d(output)).\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the gradient with respect to the input x (dL/dx).\n",
    "        \"\"\"\n",
    "        # Recompute tanh(x) - necessary as the forward output isn't directly available here.\n",
    "        # Use the stable computation method.\n",
    "        tanh_x_value = TanH.TanHCompute(x.value)\n",
    "\n",
    "        # Compute the local derivative: d(tanh(x))/dx = 1 - tanh(x)^2\n",
    "        local_derivative = 1.0 - tanh_x_value**2\n",
    "\n",
    "        # Apply chain rule: dL/dx = dL/d(output) * d(output)/dx\n",
    "        grad_input = gradient * local_derivative\n",
    "\n",
    "        # Return as a tuple\n",
    "        return (grad_input,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d873cf-a10f-4e33-ac84-0018933ab4c5",
   "metadata": {},
   "source": [
    "**Question 2:** Next, we implement the affine transform operation.\n",
    "You can reuse the code from the third lab exercise, with one major difference: you have to compute the gradient with respect to x too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95cf9932-ddcf-472f-8732-c645b5063cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class affine_transform(Operation):\n",
    "    @staticmethod\n",
    "    def forward(W, b, x):\n",
    "        \"\"\"\n",
    "        Computes the forward pass of an affine transform: y = x @ W + b.\n",
    "\n",
    "        Args:\n",
    "            W (Parameter): Weight matrix node, shape (D_in, D_out).\n",
    "            b (Parameter): Bias vector node, shape (1, D_out) or (D_out,).\n",
    "            x (ComputationGraphNode): Input node, shape (N, D_in).\n",
    "\n",
    "        Returns:\n",
    "            ComputationGraphNode: Output node y, shape (N, D_out).\n",
    "        \"\"\"\n",
    "        # Get numpy values from input nodes\n",
    "        np_W = W.value\n",
    "        np_b = b.value\n",
    "        np_x = x.value\n",
    "\n",
    "        # Validate shapes for matrix multiplication\n",
    "        if np_x.shape[-1] != np_W.shape[0]:\n",
    "             raise ValueError(f\"Shape mismatch for x @ W: x shape {np_x.shape}, W shape {np_W.shape}\")\n",
    "\n",
    "        # Perform the affine transformation\n",
    "        # Using @ operator for matrix multiplication (preferred in Python 3.5+)\n",
    "        try:\n",
    "            output_array = np_x @ np_W + np_b\n",
    "        except ValueError as e:\n",
    "             # Catch potential broadcasting issues with bias\n",
    "             raise ValueError(f\"Error during affine transform x({np_x.shape}) @ W({np_W.shape}) + b({np_b.shape}): {e}\")\n",
    "\n",
    "\n",
    "        # Create the output node\n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        # IMPORTANT: Store input nodes in the order W, b, x for backward pass\n",
    "        output_node.set_input_nodes(W, b, x)\n",
    "        # The Operation.__call__ method will set output_node.set_func(self)\n",
    "\n",
    "        return output_node\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(W, b, x, gradient):\n",
    "        \"\"\"\n",
    "        Computes the backward pass for the affine transform.\n",
    "\n",
    "        Args:\n",
    "            W (Parameter): Weight matrix node from forward pass.\n",
    "            b (Parameter): Bias vector node from forward pass.\n",
    "            x (ComputationGraphNode): Input node from forward pass.\n",
    "            gradient (np.ndarray): Gradient flowing back from the next layer (dL/dy), shape (N, D_out).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Gradients (dL/dW, dL/db, dL/dx).\n",
    "        \"\"\"\n",
    "        # Get numpy values needed for gradient calculations\n",
    "        np_W = W.value # Shape (D_in, D_out)\n",
    "        np_x = x.value # Shape (N, D_in)\n",
    "        # np_b = b.value # Bias value itself not needed for derivative calculations\n",
    "\n",
    "        # --- Calculate dL/dW ---\n",
    "        # dL/dW = x.T @ (dL/dy)\n",
    "        try:\n",
    "            grad_W = np_x.T @ gradient # Shape: (D_in, N) @ (N, D_out) -> (D_in, D_out)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Shape mismatch calculating grad_W: x.T({np_x.T.shape}) @ grad({gradient.shape}): {e}\")\n",
    "\n",
    "\n",
    "        # --- Calculate dL/db ---\n",
    "        # dL/db = sum(dL/dy over batch dimension)\n",
    "        grad_b = np.sum(gradient, axis=0) # Shape: (D_out,)\n",
    "        # Ensure grad_b shape matches b.value's shape for accumulation\n",
    "        # If b.value was (1, D_out), reshape grad_b\n",
    "        if b.value.ndim == 2 and b.value.shape[0] == 1:\n",
    "             if grad_b.ndim == 1: # Check if sum reduced dimension\n",
    "                 grad_b = grad_b.reshape(1, -1) # Reshape to (1, D_out)\n",
    "        # If b.value was (D_out,), grad_b is likely already (D_out,)\n",
    "\n",
    "\n",
    "        # --- Calculate dL/dx ---\n",
    "        # dL/dx = (dL/dy) @ W.T\n",
    "        try:\n",
    "            grad_x = gradient @ np_W.T # Shape: (N, D_out) @ (D_out, D_in) -> (N, D_in)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Shape mismatch calculating grad_x: grad({gradient.shape}) @ W.T({np_W.T.shape}): {e}\")\n",
    "\n",
    "\n",
    "        # Return gradients in the order corresponding to the inputs (W, b, x)\n",
    "        return grad_W, grad_b, grad_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372bea6-9480-4c5c-b3c4-0bdc570abecb",
   "metadata": {},
   "source": [
    "**Question 3:** Define the NLL operation\n",
    "\n",
    "We recall that \n",
    "$$nll(x, y)= -log\\left(\\frac{e^{x_{y}}}{ \\sum\\limits_{i=1}^n e^{x_{ j}} }\\right) = -x_{y} + log(\\sum\\limits_{i=1}^n e^{x_{ j} })$$\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial nll(x, y)}{\\partial x_i} &= - \\mathbb{1}_{y = i} + \\frac{\\partial log(\\sum\\limits_{i=1}^n e^{x_{ j} })}{\\partial\\sum\\limits_{i=1}^n e^{x_{ j} }}\\frac{\\sum\\limits_{i=1}^n e^{x_{ j} }}{\\partial x_i} \\\\\n",
    "        &= - \\mathbb{1}_{y = i} + \\frac{e^{x_i}}{\\sum\\limits_{i=1}^n e^{x_{ j} }} \n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b3f742f0-83f3-4983-ae7a-cefd511b96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume ComputationGraphNode, Operation are defined as in the prompt\n",
    "\n",
    "class nll(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, y):\n",
    "        np_x = x.value # Shape (N, C)\n",
    "        np_y = y.value.flatten().astype(int) # Ensure (N,) and integer type for indexing\n",
    "\n",
    "        N = np_x.shape[0]\n",
    "        if N == 0: # Handle empty batch\n",
    "            return ComputationGraphNode(np.array([]))\n",
    "        if np_y.shape[0] != N:\n",
    "            raise ValueError(f\"Batch size mismatch between x ({N}) and y ({np_y.shape[0]})\")\n",
    "\n",
    "        # --- Numerically Stable Log-Sum-Exp ---\n",
    "        # Find max logit for each sample for stability\n",
    "        max_logits = np.max(np_x, axis=1, keepdims=True) # Shape (N, 1)\n",
    "        # Subtract max, exponentiate, sum, log, add max back\n",
    "        stable_x = np_x - max_logits # Shape (N, C)\n",
    "        log_sum_exp = np.log(np.sum(np.exp(stable_x), axis=1, keepdims=True)) + max_logits # Shape (N, 1)\n",
    "\n",
    "        # --- Select the logit corresponding to the true class y ---\n",
    "        # Use advanced indexing to get x[n, y[n]] for each n in N\n",
    "        logits_of_true_class = np_x[np.arange(N), np_y].reshape(N, 1) # Shape (N, 1)\n",
    "\n",
    "        # --- Compute NLL ---\n",
    "        # nll = -x_y + log(sum(exp(x_j)))\n",
    "        loss_array = -logits_of_true_class + log_sum_exp # Shape (N, 1)\n",
    "\n",
    "        # Create output node, typically loss per sample (N,)\n",
    "        output_node = ComputationGraphNode(loss_array.flatten()) # Shape (N,)\n",
    "        output_node.set_input_nodes(x, y)\n",
    "        # Store necessary values for backward? The formula only needs x and y.\n",
    "        # No extra func_parameters needed here based on the formula.\n",
    "\n",
    "        return output_node\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x, y, gradient):\n",
    "        np_x = x.value # Shape (N, C)\n",
    "        np_y = y.value.flatten().astype(int) # Shape (N,)\n",
    "\n",
    "        N, C = np_x.shape\n",
    "        if N == 0: # Handle empty batch\n",
    "             return (np.zeros_like(np_x), None)\n",
    "\n",
    "        # --- Numerically Stable Softmax ---\n",
    "        max_logits = np.max(np_x, axis=1, keepdims=True) # Shape (N, 1)\n",
    "        stable_x = np_x - max_logits # Shape (N, C)\n",
    "        exp_x = np.exp(stable_x) # Shape (N, C)\n",
    "        sum_exp_x = np.sum(exp_x, axis=1, keepdims=True) # Shape (N, 1)\n",
    "        softmax_x = exp_x / sum_exp_x # Shape (N, C)\n",
    "\n",
    "        # --- Gradient Calculation (d(nll)/dx = softmax(x) - indicator(y=i)) ---\n",
    "        # Create the indicator matrix (one-hot encoding of y)\n",
    "        indicator = np.zeros_like(np_x) # Shape (N, C)\n",
    "        indicator[np.arange(N), np_y] = 1.0\n",
    "\n",
    "        # Calculate the local gradient d(nll)/dx\n",
    "        grad_x_local = softmax_x - indicator # Shape (N, C)\n",
    "\n",
    "        # --- Apply Chain Rule: dL/dx = dL/d(nll_output) * d(nll)/dx ---\n",
    "        # The incoming gradient is dL/d(nll_output).\n",
    "        # If nll.forward returned (N,), gradient should be (N,) or broadcastable (scalar)\n",
    "        if gradient.ndim == 0: # Scalar gradient (e.g., 1.0 or 1.0/N)\n",
    "             grad_x = gradient * grad_x_local\n",
    "        elif gradient.ndim == 1 and gradient.shape[0] == N:\n",
    "             # gradient shape (N,), grad_x_local shape (N, C)\n",
    "             # Multiply each sample's gradient with its local gradient vector\n",
    "             grad_x = gradient[:, np.newaxis] * grad_x_local # (N, 1) * (N, C) -> (N, C)\n",
    "        else:\n",
    "             raise ValueError(f\"Gradient shape {gradient.shape} incompatible with NLL output ({N},)\")\n",
    "\n",
    "\n",
    "        # Gradient w.r.t y is not defined/needed\n",
    "        grad_y = None\n",
    "\n",
    "        return (grad_x, grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016217fb-a9f4-4908-b3e7-5fe372b38946",
   "metadata": {},
   "source": [
    "# Module\n",
    "\n",
    "Neural networks or parts of neural networks will be stored in Modules.\n",
    "They implement method to retrieve all parameters of the network and subnetwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43ea790d-a625-4aa4-95f1-8e4fe5e558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def parameters(self):\n",
    "        ret = []\n",
    "        for name in dir(self):\n",
    "            o = self.__getattribute__(name)\n",
    "\n",
    "            if type(o) is Parameter:\n",
    "                ret.append(o)\n",
    "            if isinstance(o, Module) or isinstance(o, ModuleList):\n",
    "                ret.extend(o.parameters())\n",
    "        return ret\n",
    "\n",
    "# if you want to store a list of Parameters or Module,\n",
    "# you must store them in a ModuleList instead of a python list,\n",
    "# in order to collect the parameters correctly\n",
    "class ModuleList(list):\n",
    "    def parameters(self):\n",
    "        ret = []\n",
    "        for m in self:\n",
    "            if type(m) is Parameter:\n",
    "                ret.append(m)\n",
    "            elif isinstance(m, Module) or isinstance(m, ModuleList):\n",
    "                ret.extend(m.parameters())\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9688649-c900-4f00-bd06-38d93c1249cf",
   "metadata": {},
   "source": [
    "# Initialization and optimization\n",
    "\n",
    "**Question 1:** Implement the different initialisation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "337e899b-7551-42f5-ba48-bfef526f86a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(b):\n",
    "    \"\"\"Initializes a NumPy array (typically bias) with zeros in-place.\"\"\"\n",
    "    # Ensure it's a float array if you want float zeros\n",
    "    if not np.issubdtype(b.dtype, np.floating):\n",
    "        b[:] = 0.0\n",
    "    else:\n",
    "        b.fill(0) # Efficient way to fill with zeros\n",
    "\n",
    "def glorot_init(W):\n",
    "    \"\"\"\n",
    "    In-place initialization of a weight matrix W using Glorot (Xavier) uniform method.\n",
    "    Suitable for layers followed by symmetric activations like tanh.\n",
    "    \"\"\"\n",
    "    if W.ndim != 2:\n",
    "        raise ValueError(\"Glorot initialization expects a 2D weight matrix.\")\n",
    "\n",
    "    fan_in, fan_out = W.shape[0], W.shape[1]\n",
    "    if fan_in + fan_out == 0: # Avoid division by zero for empty layers\n",
    "        W[:, :] = 0.0 # Or handle as appropriate\n",
    "        return\n",
    "\n",
    "    # Calculate the limit for the uniform distribution\n",
    "    limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "\n",
    "    # Generate random values from Uniform(-limit, limit) and assign in-place\n",
    "    W[:, :] = np.random.uniform(low=-limit, high=limit, size=W.shape)\n",
    "\n",
    "def kaiming_init(W):\n",
    "    \"\"\"\n",
    "    In-place initialization of a weight matrix W using Kaiming (He) normal method.\n",
    "    Suitable for layers followed by ReLU activations. Assumes fan_in mode.\n",
    "    \"\"\"\n",
    "    if W.ndim != 2:\n",
    "        raise ValueError(\"Kaiming initialization expects a 2D weight matrix.\")\n",
    "\n",
    "    fan_in = W.shape[0]\n",
    "    if fan_in == 0: # Avoid division by zero for empty layers\n",
    "        W[:, :] = 0.0 # Or handle as appropriate\n",
    "        return\n",
    "\n",
    "    # Calculate the standard deviation for the normal distribution\n",
    "    # Assumes nonlinearity is ReLU (gain=sqrt(2))\n",
    "    stddev = np.sqrt(2.0 / fan_in)\n",
    "\n",
    "    # Generate random values from Normal(0, stddev) and assign in-place\n",
    "    W[:, :] = np.random.normal(loc=0.0, scale=stddev, size=W.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ee98a-e90a-4ddb-b575-1688371efc05",
   "metadata": {},
   "source": [
    "We will implement the Stochastic gradient descent through an object, in the init function this object will store the different parameters (in a list format). The step function will update the parameters (see slides), notice that the gradient is stored in the nodes (grad attribute). Finally it will be necessary after each update to reset all the gradient to zero (in the method zero_grad) because we do not want to accumumlate gradient of all previous step.\n",
    "\n",
    "**Question 2:** Implement the SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dbb7d09f-f4ae-472f-8b6a-f39fbaf8dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    Implements stochastic gradient descent (optionally with momentum).\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the SGD optimizer.\n",
    "\n",
    "        Args:\n",
    "            params (list): A list of Parameter objects to optimize.\n",
    "            lr (float): Learning rate.\n",
    "        \"\"\"\n",
    "        if not isinstance(params, list):\n",
    "            raise TypeError(\"params must be a list of Parameter objects.\")\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step (parameter update).\n",
    "        \"\"\"\n",
    "        for p in self.params:\n",
    "            # Check if the parameter requires gradient and if gradient exists\n",
    "            if p.require_grad and p.grad is not None:\n",
    "                # Ensure grad and value are numpy arrays for the operation\n",
    "                if not isinstance(p.value, np.ndarray) or not isinstance(p.grad, np.ndarray):\n",
    "                     print(f\"Warning: Parameter {getattr(p, 'name', 'unnamed')} value or grad is not a numpy array. Skipping update.\")\n",
    "                     continue\n",
    "                try:\n",
    "                    # Perform the SGD update: param = param - learning_rate * gradient\n",
    "                    # Use -= for potential in-place update if possible with numpy arrays\n",
    "                    p.value -= self.lr * p.grad\n",
    "                except (TypeError, ValueError) as e:\n",
    "                     # Catch potential issues like shape mismatch or dtype problems\n",
    "                     print(f\"Warning: Error updating parameter {getattr(p, 'name', 'unnamed')}. \"\n",
    "                           f\"Value shape: {p.value.shape}, dtype: {p.value.dtype}. \"\n",
    "                           f\"Grad shape: {p.grad.shape}, dtype: {p.grad.dtype}. Error: {e}\")\n",
    "            # else:\n",
    "                # Parameter might not require grad, or grad might be None (e.g., if not part of graph)\n",
    "                # No update needed in these cases.\n",
    "                # Optionally add a check/warning if require_grad=True but grad is None after backward.\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Resets the gradients of all parameters managed by the optimizer to zero.\n",
    "        \"\"\"\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                # Check if grad is a numpy array before calling fill\n",
    "                if hasattr(p.grad, 'fill'):\n",
    "                    try:\n",
    "                        # Use fill(0) for efficient in-place zeroing of numpy arrays\n",
    "                        p.grad.fill(0)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not zero gradient for param {getattr(p, 'name', 'unnamed')} using fill(). Error: {e}. Trying assignment.\")\n",
    "                        # Fallback: Assign a new zero array\n",
    "                        try:\n",
    "                            p.grad = np.zeros_like(p.grad)\n",
    "                        except Exception as e_assign:\n",
    "                             print(f\"Error: Could not assign zeros to gradient for param {getattr(p, 'name', 'unnamed')}. Error: {e_assign}\")\n",
    "\n",
    "                else:\n",
    "                    # If grad is not a numpy array (shouldn't happen with Parameter), try assigning 0.0\n",
    "                    try:\n",
    "                         p.grad = 0.0 # Or appropriate zero value based on expected type\n",
    "                    except Exception as e_assign_scalar:\n",
    "                        print(f\"Warning: Gradient for param {getattr(p, 'name', 'unnamed')} is not a numpy array and could not be zeroed. Type: {type(p.grad)}. Error: {e_assign_scalar}\")\n",
    "            # If p.grad is None, nothing to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76263efd-aed5-4a9a-b954-d9df83c9e8f4",
   "metadata": {},
   "source": [
    "# Networks and training loop\n",
    "\n",
    "We first create a simple linear classifier, similar to the first lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "50268482-0561-4e42-a85d-d7f592ca9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(Module):\n",
    "    \"\"\"\n",
    "    A simple linear layer module (affine transformation).\n",
    "    Applies the transformation y = x @ W + b.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_input, dim_output):\n",
    "        \"\"\"\n",
    "        Initializes the linear layer.\n",
    "\n",
    "        Args:\n",
    "            dim_input (int): Dimensionality of the input features.\n",
    "            dim_output (int): Dimensionality of the output features.\n",
    "        \"\"\"\n",
    "        super().__init__() # Initialize base Module class\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_output = dim_output\n",
    "\n",
    "        # Build the parameters W and b\n",
    "        # Create numpy arrays first (can be empty, init_parameters will fill them)\n",
    "        # Using float for typical network operations\n",
    "        W_data = np.empty((dim_input, dim_output), dtype=float)\n",
    "        b_data = np.empty((1, dim_output), dtype=float) # Bias shape (1, D_out) for broadcasting\n",
    "\n",
    "        # Wrap numpy arrays in Parameter nodes\n",
    "        self.W = Parameter(W_data, name=f'Linear_{dim_input}x{dim_output}_W')\n",
    "        self.b = Parameter(b_data, name=f'Linear_{dim_input}x{dim_output}_b')\n",
    "\n",
    "        # Note: Actual value initialization happens in init_parameters\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializes the weight (W) and bias (b) parameters of the network.\n",
    "        Uses Glorot initialization for weights and zero initialization for biases.\n",
    "        \"\"\"\n",
    "        # Use Glorot initialization for the weight matrix W\n",
    "        # It modifies the self.W.value array in-place\n",
    "        glorot_init(self.W.value)\n",
    "\n",
    "        # Use zero initialization for the bias vector b\n",
    "        # It modifies the self.b.value array in-place\n",
    "        zero_init(self.b.value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass: computes x @ W + b.\n",
    "\n",
    "        Args:\n",
    "            x (ComputationGraphNode): Input node, expected shape (N, dim_input).\n",
    "\n",
    "        Returns:\n",
    "            ComputationGraphNode: Output node, shape (N, dim_output).\n",
    "        \"\"\"\n",
    "        # Ensure x is a ComputationGraphNode\n",
    "        if not isinstance(x, ComputationGraphNode):\n",
    "            # Depending on strictness, either raise error or wrap it\n",
    "            # Let's wrap it for flexibility, assuming x is a numpy array here\n",
    "            x = ComputationGraphNode(x)\n",
    "            # Note: If x doesn't require grad, this might be slightly inefficient\n",
    "            # but necessary for the graph structure.\n",
    "\n",
    "        # Apply the affine_transform operation\n",
    "        # Pass the Parameter nodes (self.W, self.b) and the input node (x)\n",
    "        # The operation returns the resulting ComputationGraphNode\n",
    "        output_node = affine_transform()(self.W, self.b, x)\n",
    "        return output_node\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"Provides a string representation of the module.\"\"\"\n",
    "        return f\"LinearNetwork(dim_input={self.dim_input}, dim_output={self.dim_output})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "49500117-74d1-4012-9096-53e3b8f298fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "aaf06f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of input x (first 10 elements):\n",
      "[-0.02910252  0.26027206  0.0721708  -0.49305239 -0.07406743 -0.34937228\n",
      "  0.10215195 -0.08538571  0.29589588 -0.07565649]\n"
     ]
    }
   ],
   "source": [
    "# those lines should be executed correctly\n",
    "lin1 = LinearNetwork(784, 10)\n",
    "lin2 = LinearNetwork(10, 5)\n",
    "\n",
    "lin1.init_parameters()\n",
    "lin2.init_parameters()\n",
    "\n",
    "input_image = train_data[0][0]\n",
    "if input_image.ndim == 1:\n",
    "    input_image = input_image.reshape(1, -1)\n",
    "\n",
    "x = ComputationGraphNode(input_image, require_grad=True)\n",
    "\n",
    "a = lin1.forward(Addition()(x, x))\n",
    "b = TanH()(a)\n",
    "c = lin2.forward(b)\n",
    "c.backward()\n",
    "\n",
    "print(\"Gradient of input x (first 10 elements):\")\n",
    "if x.grad is not None:\n",
    "    print(x.grad.flatten()[:10])\n",
    "else:\n",
    "    print(\"x.grad is None (check require_grad and backward pass)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b40f7e-5c56-4c49-bcc7-82b27730e3b6",
   "metadata": {},
   "source": [
    "We will train several neural networks.\n",
    "Therefore, we encapsulate the training loop in a function.\n",
    "\n",
    "**warning**: you have to call optimizer.zero_grad() before each backward pass to reinitialize the gradient of the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "557d5a9a-c5b3-455b-9084-20470d1901db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(network, optimizer, train_data, dev_data, n_epochs=10):\n",
    "    X_train, y_train = train_data  # Training data: (inputs, labels)\n",
    "    X_dev, y_dev = dev_data        # Dev data: (inputs, labels)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        train_predictions = network.forward(X_train)\n",
    "        y_train_node = ComputationGraphNode(y_train)\n",
    "        \n",
    "        # Use the __call__ interface of nll to attach the operation to the node.\n",
    "        loss_node = nll()(train_predictions, y_train_node)\n",
    "        mean_loss = np.mean(loss_node.value)\n",
    "        \n",
    "        # Use a gradient with the same shape as loss_node.value.\n",
    "        N = len(loss_node.value)\n",
    "        loss_node.backward(np.ones_like(loss_node.value) / N)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        dev_predictions = network.forward(X_dev)\n",
    "        predicted_labels = np.argmax(dev_predictions.value, axis=1)\n",
    "        accuracy = np.mean(predicted_labels == y_dev)\n",
    "        \n",
    "        print(f\"mean loss -> {mean_loss} validation accuracy -> {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0d3e4305-1736-4987-a5d6-d0902dc78738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss -> 2.4296296662456505 validation accuracy -> 0.0721\n",
      "mean loss -> 2.4156190269698405 validation accuracy -> 0.0755\n",
      "mean loss -> 2.401992674947218 validation accuracy -> 0.0811\n",
      "mean loss -> 2.388717143607291 validation accuracy -> 0.0852\n",
      "mean loss -> 2.3757632291864654 validation accuracy -> 0.0891\n"
     ]
    }
   ],
   "source": [
    "dim_input = 28*28\n",
    "dim_output = 10\n",
    "\n",
    "network = LinearNetwork(dim_input, dim_output)\n",
    "network.init_parameters()\n",
    "optimizer = SGD(network.parameters(), 0.01)\n",
    "\n",
    "training_loop(network, optimizer, train_data, dev_data, n_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab8c45-1492-4be0-a3bd-d257066b5876",
   "metadata": {},
   "source": [
    "After you finished the linear network, you can move to a deep network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3180e14e-6490-4008-8fda-a1f7f7f71bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(Module):\n",
    "    def __init__(self, dim_input, dim_output, hidden_dim, n_layers, tanh=False):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        dim_input  : number of input features\n",
    "        dim_output : number of output units/classes\n",
    "        hidden_dim : dimension of each hidden layer\n",
    "        n_layers   : number of hidden layers\n",
    "        tanh       : if True, use tanh activation; else use ReLU\n",
    "        \"\"\"\n",
    "        # We keep a list of layers in a ModuleList so the parameters can be collected:\n",
    "        self.layers = ModuleList()\n",
    "        self.use_tanh = tanh\n",
    "\n",
    "        # 1) First hidden layer: input -> hidden_dim\n",
    "        self.layers.append(LinearNetwork(dim_input, hidden_dim))\n",
    "\n",
    "        # 2) Next hidden layers: hidden_dim -> hidden_dim\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(LinearNetwork(hidden_dim, hidden_dim))\n",
    "\n",
    "        # 3) Final layer: hidden_dim -> dim_output\n",
    "        self.layers.append(LinearNetwork(hidden_dim, dim_output))\n",
    "\n",
    "        # Initialize all parameters right after construction\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize parameters of each sub-layer\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.init_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: \n",
    "          - For all hidden layers, apply linear => activation\n",
    "          - For the final layer, apply linear only (no activation)\n",
    "        \"\"\"\n",
    "        # Pass through the first N hidden layers with activation\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer.forward(x)\n",
    "            if self.use_tanh:\n",
    "                x = TanH()(x)\n",
    "            else:\n",
    "                x = ReLU()(x)\n",
    "\n",
    "        # Final layer: produce raw outputs (e.g. logits)\n",
    "        x = self.layers[-1].forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "54b2e256-b2fe-4e7f-8f5d-777ba54cc7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean loss -> 2.3261267912328973 validation accuracy -> 0.1751\n",
      "mean loss -> 2.3189201776771773 validation accuracy -> 0.1804\n",
      "mean loss -> 2.311857649368916 validation accuracy -> 0.1869\n",
      "mean loss -> 2.3049334367056944 validation accuracy -> 0.1914\n",
      "mean loss -> 2.2981409542554316 validation accuracy -> 0.1954\n"
     ]
    }
   ],
   "source": [
    "dim_input = 28*28\n",
    "dim_output = 10\n",
    "\n",
    "network = DeepNetwork(dim_input, dim_output, 100, 2)\n",
    "network.init_parameters()\n",
    "optimizer = SGD(network.parameters(), 0.01)\n",
    "\n",
    "training_loop(network, optimizer, train_data, dev_data, n_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af7721-8e42-4b4f-aa58-fe3b02d3d867",
   "metadata": {},
   "source": [
    "## Better Optimizer\n",
    "Implement the SGD with momentum, notice that you will need to store the cumulated gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b804a3fc-9136-4d2f-8f18-a394821401de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDWithMomentum:\n",
    "    def __init__(self, params, lr=0.1, momentum=0.5):\n",
    "        \"\"\"\n",
    "        params: a list of Parameter objects\n",
    "        lr: learning rate (float)\n",
    "        momentum: momentum factor (float)\n",
    "        \"\"\"\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # We store a velocity array for each parameter,\n",
    "        # initialized to zeros of the same shape.\n",
    "        self.velocities = []\n",
    "        for p in params:\n",
    "            self.velocities.append(np.zeros_like(p.value))\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Apply one update step to each parameter.\n",
    "        velocity = momentum * velocity - lr * grad\n",
    "        param = param + velocity\n",
    "        \"\"\"\n",
    "        for i, p in enumerate(self.params):\n",
    "            if p.grad is not None:\n",
    "                # Update velocity\n",
    "                self.velocities[i] = self.momentum * self.velocities[i] - self.lr * p.grad\n",
    "                # Update parameter\n",
    "                p.value += self.velocities[i]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Reset the accumulated gradients on each parameter to zero.\n",
    "        \"\"\"\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.fill(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8739e57-a82b-4908-9488-1b83d6e9a51f",
   "metadata": {},
   "source": [
    "## Bonus: Batch SGD\n",
    "Propose a methods to take into account batch of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824876ff-1dca-44ec-a777-620dfdd0d0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
